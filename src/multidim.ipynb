{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidimensional Analysis of Synaptic Proteins\n",
    "\n",
    "In this notebook we go through the multidimensional analyis of synaptic proteins that was created for the following paper : _Activity-dependent remodelling of synaptic protein organization revealed by high throughput analysis of STED nanoscopy images_.\n",
    "\n",
    "Here are the sections of the analysis:\n",
    "* [Define functions](#define-functions)\n",
    "* [Define constants](#define-constants)\n",
    "* [Dimensionality reduction from UMAP](#reduce-umap)\n",
    "* [Hierarchical clustering](#hierarchical-clustering)\n",
    "* [Associate cluster](#associate-cluster)\n",
    "\n",
    "__Please cite this analysis pipeline using the following bibtex entry__.\n",
    "```\n",
    "@article{Wiesner2020,\n",
    "  title={Activity-dependent remodelling of synaptic protein organization revealed by high throughput analysis of STED nanoscopy images},\n",
    "  author={Wiesner, Theresa and Bilodeau, Anthony and Bernatchez, Renaud and Raulier, Bastian and De Koninck, Paul and Lavoie-Cardinal, Flavie},\n",
    "  journal={Frontiers in Neural Circuits},\n",
    "  volume={},\n",
    "  number={},\n",
    "  pages={},\n",
    "  year={2020},\n",
    "  publisher={Frontiers}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy \n",
    "import os\n",
    "import warnings\n",
    "import matplotlib \n",
    "import pickle \n",
    "import random\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import defaultdict\n",
    "from umap import UMAP\n",
    "from matplotlib import pyplot, lines, gridspec\n",
    "from seaborn.distributions import _statsmodels_bivariate_kde\n",
    "from skimage import feature\n",
    "\n",
    "# Sets random seed for reproducibility\n",
    "random.seed(42)\n",
    "numpy.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='define-functions'></a>\n",
    "## Define functions \n",
    "\n",
    "The following functions are used throughout the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(plasticity, condition, chan, keep_features):\n",
    "    \"\"\"\n",
    "    Loads the data and only keeps the coupled clusters\n",
    "    \n",
    "    :param plasticity: A `str` of the plasticity\n",
    "    :param condition: A `str` of the condition to load \n",
    "    :param chan: An `int` of the channel to load\n",
    "    :param keep_features: A `list` of integers of features to keep from file\n",
    "    \n",
    "    :returns : A `numpy.ndarray` of the data \n",
    "    \"\"\"\n",
    "    data = numpy.load(f\"./data/{plasticity}-plasticity-pfile/{condition}-chan{chan}.npy\", allow_pickle=True)\n",
    "    \n",
    "    X = []\n",
    "    for _data in data:\n",
    "        coupled = _data[:, 13].astype(bool)\n",
    "        coupled = _data[coupled, :]\n",
    "        coupled = coupled[:, keep_features]\n",
    "\n",
    "        X.append(coupled)\n",
    "    return numpy.concatenate(X, axis=0)\n",
    "\n",
    "class Grid:\n",
    "    \"\"\"\n",
    "    Creates a `Grid` object to store each data point in a specific cell.\n",
    "    This allows to then compute statistics on specific region of the\n",
    "    data points.\n",
    "    \"\"\"\n",
    "    def __init__(self, xlim, ylim, gridsize):\n",
    "        \"\"\"\n",
    "        Instantiate a `Grid` object\n",
    "\n",
    "        :param xlim: A `tuple` of x limits\n",
    "        :param ylim: A `tuple` of y limits\n",
    "        :param gridsize: The number of cells along each axis\n",
    "        \"\"\"\n",
    "        self.xlim = xlim\n",
    "        self.ylim = ylim\n",
    "        self.gridsize = gridsize\n",
    "\n",
    "        self.grid = self.create_grid()\n",
    "        self.xticks = numpy.linspace(*xlim, gridsize)\n",
    "        self.yticks = numpy.linspace(*ylim, gridsize)\n",
    "\n",
    "    def clear_grid(self):\n",
    "        \"\"\"\n",
    "        Method to clear the grid\n",
    "        \"\"\"\n",
    "        self.grid = self.create_grid()\n",
    "\n",
    "    def create_grid(self):\n",
    "        \"\"\"\n",
    "        Method to create an empty grid\n",
    "\n",
    "        :returns : An empty grid\n",
    "        \"\"\"\n",
    "        grid = [[[] for _ in range(self.gridsize)] \\\n",
    "                    for _ in range(self.gridsize)]\n",
    "        return grid\n",
    "\n",
    "    def add_data(self, ijk):\n",
    "        \"\"\"\n",
    "        Method to add data points to the grid\n",
    "\n",
    "        :param ijk: A list of `tuple` with (x, y) coordinates and value to append\n",
    "        \"\"\"\n",
    "        for i, j, k in ijk:\n",
    "            i_index = int((i - self.xlim[0]) / ((self.xlim[1] - self.xlim[0]) / self.gridsize))\n",
    "            j_index = int((j - self.ylim[0]) / ((self.ylim[1] - self.ylim[0]) / self.gridsize))\n",
    "            if (i_index < 0) or (i_index >= self.gridsize) or (j_index < 0) or (j_index >= self.gridsize):\n",
    "                continue\n",
    "            self.grid[j_index][i_index].append(k)\n",
    "\n",
    "    def compute_stats(self, method=None):\n",
    "        \"\"\"\n",
    "        Method to compute statistics on every cell of the grid\n",
    "\n",
    "        :param method: (optional) A `function` to compute statistics. This `function`\n",
    "                       should accept `list` as input. If None `numpy.mean` is selected\n",
    "\n",
    "        :returns : A `numpy.ndarray` of the statistics on each cell of the grid\n",
    "        \"\"\"\n",
    "        if isinstance(method, (type(None))) or (not isinstance(method, (type(numpy.mean)))):\n",
    "            method = numpy.mean\n",
    "        output_grid = []\n",
    "        for rows in self.grid:\n",
    "            output_cols = []\n",
    "            for cols in rows:\n",
    "                output_cols.append(method(cols) if cols else 0)\n",
    "            output_grid.append(output_cols)\n",
    "        return numpy.array(output_grid)\n",
    "\n",
    "def get_norm(keep_features):\n",
    "    \"\"\"\n",
    "    Computes the normalization of all features for all data with 0.05 and 0.95 quantiles\n",
    "    \n",
    "    :param keep_features: A `list` of integers of features to keep \n",
    "    \n",
    "    :param : A `numpy.ndarray` of minimal values for each features\n",
    "             A `numpy.ndarray` of maximal values for each features\n",
    "    \"\"\"\n",
    "    conditions = [\n",
    "        (\"synaptic\", \"block\"),\n",
    "        (\"synaptic\", \"0mgglybic\"),\n",
    "        (\"synaptic\", \"glugly\"),\n",
    "        (\"homeostatic\", \"ctrl\"),\n",
    "        (\"homeostatic\", \"4hTTX\"),\n",
    "        (\"homeostatic\", \"24hTTX\"),\n",
    "        (\"homeostatic\", \"48hTTX\")\n",
    "    ]\n",
    "    all_data = []\n",
    "    for chan in [1, 2]:\n",
    "        for (plasticity, cond) in conditions:\n",
    "            data = load_data(plasticity, cond, chan, keep_features)\n",
    "            all_data.append(data)\n",
    "    all_data = numpy.concatenate(all_data, axis=0)\n",
    "    vmin, vmax = numpy.quantile(all_data, q=[0.05, 0.95], axis=0)\n",
    "    return vmin, vmax\n",
    "\n",
    "def associate(subtypes, labels, plasticity, condition, chan, keep_features):\n",
    "    \"\"\"\n",
    "    Associate each cluster with averaged cluster maximas\n",
    "    \n",
    "    :param subtypes: A `numpy.ndarray` of size (N, len(keep_features))\n",
    "    :param labels: A `numpy.ndarray` of size (N,) of the labels associated for each average cluster\n",
    "    :param plasticity: A `str` of the plasticity\n",
    "    :param condition: A `str` of the condition to load \n",
    "    :param chan: An `int` of the channel to load\n",
    "    :param keep_features: A `list` of integers of features to keep from file\n",
    "    \n",
    "    :returns : Number of synaptic subtypes per image\n",
    "               Distance of association\n",
    "    \"\"\"\n",
    "    data = numpy.load(f\"./data/{plasticity}-plasticity-pfile/{condition}-chan{chan}.npy\", allow_pickle=True)\n",
    "\n",
    "    subtypes = (subtypes - _min) / (_max - _min)\n",
    "    density_per_image, association_distance = [], []\n",
    "    for _data in data:        \n",
    "        coupled = _data[:, 13].astype(bool)\n",
    "        _data = _data[coupled, :]\n",
    "        _data = _data[:, :]\n",
    "        \n",
    "        tmp = (_data[:, keep_features] - _min) / (_max - _min)\n",
    "        distances = distance.cdist(subtypes, tmp, metric=\"euclidean\")\n",
    "        associated_subtype = labels[numpy.argmin(distances, axis=0)]\n",
    "        ids, counts = numpy.unique(associated_subtype, return_counts=True)\n",
    "        number_subtype = numpy.zeros(labels.max() + 1)\n",
    "        number_subtype[ids] += counts\n",
    "        density_per_image.append(number_subtype)\n",
    "        \n",
    "        association_distance.extend(numpy.min(distances, axis=0))\n",
    "\n",
    "    return numpy.array(density_per_image), association_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='define-constants'></a>\n",
    "## Define constants\n",
    "\n",
    "We define constants that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the default cmap instance\n",
    "CMAP = pyplot.cm.YlGnBu\n",
    "\n",
    "# Defines the channel to analyse {1, 2}\n",
    "CHAN = 1\n",
    "\n",
    "# Defines wheter the users want to save the trained mapper\n",
    "SAVE_MAPPER = False\n",
    "\n",
    "# Defines the default colors \n",
    "COLORS = {\n",
    "    \"synaptic, block\" : \"black\",\n",
    "    \"synaptic, 0mgglybic\" : \"darkseagreen\",\n",
    "    \"synaptic, glugly\" : \"peru\"\n",
    "}\n",
    "\n",
    "# Defines the decision maps colormap\n",
    "DECISION_MAPS = {\n",
    "    \"block\" : matplotlib.colors.LinearSegmentedColormap.from_list(\"decisionmap\", [\"white\", COLORS[\"synaptic, block\"]]),\n",
    "    \"0mgglybic\" : matplotlib.colors.LinearSegmentedColormap.from_list(\"decisionmap\", [\"white\", COLORS[\"synaptic, 0mgglybic\"]]),\n",
    "    \"glugly\" : matplotlib.colors.LinearSegmentedColormap.from_list(\"decisionmap\", [\"white\", COLORS[\"synaptic, glugly\"]])\n",
    "}\n",
    "\n",
    "# Defines the condition to use in the notebook \n",
    "CONDITIONS = [\n",
    "    (\"synaptic\", \"block\"),\n",
    "    (\"synaptic\", \"0mgglybic\"),\n",
    "    (\"synaptic\", \"glugly\")\n",
    "]\n",
    "\n",
    "# Defines the available columns in the xlsx files\n",
    "KEYS = ['X', 'Y', 'Area', 'Distance to Neighbor Same Ch',\n",
    "       'Distance to Other Ch', 'Eccentricity', 'Max intensity',\n",
    "       'Min intensity', 'Mean intensity', 'Major axis length',\n",
    "       'Minor axis length', 'Orientation', 'Perimeter', 'Coupled', 'Coupling Prob']\n",
    "\n",
    "# Defines the features to keep\n",
    "KEEP_FEATURES = [2, 4, 5, 9, 10, 12, 14]\n",
    "\n",
    "# Defines the min and max value of the KDE plots\n",
    "VMIN, VMAX = get_norm(KEEP_FEATURES)\n",
    "VMIN[[1, 2, -1]] = [0, 0.4, 0] # Specifies min values\n",
    "VMAX[[1, 2, -1]] = [12, 1., 1] # Specifies max values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load-data'></a>\n",
    "## Load data\n",
    "\n",
    "We now load the data from the selected `CHAN` and computes the statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loads the complete data, label and colors\n",
    "X, y, c = [], [], []\n",
    "for i, (plasticity, cond) in enumerate(CONDITIONS):\n",
    "    coupled = load_data(plasticity, cond, CHAN, KEEP_FEATURES)\n",
    "\n",
    "    X.append(coupled)\n",
    "    y.append(numpy.repeat(i, len(X[-1])))\n",
    "    c.append([COLORS[f\"{plasticity}, {cond}\"] for _ in range(len(X[-1]))])\n",
    "X, y, c = map(lambda _list : numpy.concatenate(_list, axis=0), (X, y, c))\n",
    "\n",
    "# Normalizes the data with a min max scale\n",
    "_min, _max = X.min(axis=0), X.max(axis=0)\n",
    "X = (X - _min) / (_max - _min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='reduce-umap'></a>\n",
    "## Dimensionality reduction from UMAP\n",
    "\n",
    "The users should decide wheter they want to fit a mapper from scratch or use the provided mappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit a mapper from scratch\n",
    "with warnings.catch_warnings(): # Raises a numba parallel warning\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    mapper = UMAP(n_components=2, n_neighbors=25, min_dist=0.05, init=\"spectral\", transform_seed=42)\n",
    "    mapper = mapper.fit(X)\n",
    "\n",
    "if SAVE_MAPPER:\n",
    "    pickle.dump(mapper, open(os.path.join(\"mappers\", f\"mapper-{CONDITIONS[0][0]}-{CHAN}-fit.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a previous mapper\n",
    "mapper = pickle.load(open(f\"mappers/mapper-{CONDITIONS[0][0]}-{CHAN}.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transforms the data\n",
    "X_transformed = mapper.transform(X)\n",
    "\n",
    "# Plots the transformed data on a matplotlib figure\n",
    "fig, ax = pyplot.subplots(figsize=(7, 6))\n",
    "samples = numpy.random.choice(X.shape[0], size=X.shape[0], replace=False) # Shuffles the points\n",
    "points = ax.scatter(*X_transformed[samples].T, c=c[samples], cmap=CMAP, alpha=0.8, s=10)\n",
    "norm = matplotlib.colors.Normalize(vmin=0, vmax=1)\n",
    "pyplot.colorbar(matplotlib.cm.ScalarMappable(norm=norm, cmap=DECISION_MAPS[\"block\"]), ax=ax) # creates a dummy instance of colorbar\n",
    "handles = [lines.Line2D([], [], linestyle=\"None\", marker='.', markersize=15, color=COLORS[f\"{plasticity}, {cond}\"], label=cond)\n",
    "           for i, (plasticity, cond) in enumerate(CONDITIONS)]\n",
    "ax.set_xlabel(\"UMAP 1\")\n",
    "ax.set_ylabel(\"UMAP 2\")\n",
    "pyplot.show()\n",
    "\n",
    "# Defines the xlim and ylim for all subsequent figures\n",
    "xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "\n",
    "total_maximas, total_colors = [], []\n",
    "for i, (plasticity, cond) in enumerate(CONDITIONS):\n",
    "    \n",
    "    # Density calculation\n",
    "    _x, _y = X_transformed[y==i].T\n",
    "    xx, yy, density = _statsmodels_bivariate_kde(_x, _y, bw=\"scott\", gridsize=100,\n",
    "                                                    cut=3, clip=[(-numpy.inf, numpy.inf), (-numpy.inf, numpy.inf)])\n",
    "               \n",
    "    # Finding local maxima of density \n",
    "    coordinates = feature.peak_local_max(density, min_distance=1, threshold_abs=0, indices=False)\n",
    "    xpts, ypts = xx[coordinates], yy[coordinates]\n",
    "    \n",
    "    # Defines levels to plot the contour\n",
    "    levels = numpy.linspace(0, density.max(), num=10)\n",
    "    \n",
    "    # Plots the KDE maps with countour \n",
    "    fig, ax = pyplot.subplots(figsize=(7, 6))\n",
    "    ax.scatter(xpts, ypts, c=\"r\", s=25, zorder=3)\n",
    "    cf = ax.contourf(xx, yy, density, levels=levels, cmap=DECISION_MAPS[cond])\n",
    "    ax.contour(cf, colors=\"k\")\n",
    "    norm = matplotlib.colors.Normalize(vmin=0, vmax=1)\n",
    "    pyplot.colorbar(matplotlib.cm.ScalarMappable(norm=norm, cmap=DECISION_MAPS[cond]), ax=ax)\n",
    "    handles = [lines.Line2D([], [], linestyle=\"None\", marker='.', markersize=15, color=COLORS[f\"{plasticity}, {cond}\"], label=cond)\n",
    "               for i, (plasticity, cond) in enumerate(CONDITIONS)]\n",
    "    ax.legend(handles=handles)\n",
    "    ax.set_xlabel(\"UMAP 1\")\n",
    "    ax.set_ylabel(\"UMAP 2\")\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xlim(xlim)\n",
    "    pyplot.show()\n",
    "\n",
    "    # Creates grid instance to compute local features\n",
    "    grid = Grid((xx.min(), xx.max()), (yy.min(), yy.max()), 25)\n",
    "    maxima_values, color_values = {}, {}\n",
    "    for j, kf in enumerate(KEEP_FEATURES):\n",
    "        \n",
    "        # Adds data to grid and computes stats\n",
    "        ijk = numpy.stack((_x, _y, (X * (_max - _min) + _min)[y == i, j])).T\n",
    "        grid.add_data(ijk)\n",
    "        grid_stats = grid.compute_stats(method=numpy.mean)\n",
    "        grid_stats = grid_stats.repeat(100 // 25, axis=0).repeat(100 // 25, axis=1)\n",
    "        \n",
    "        # Unnormalize data values\n",
    "        vmin = X[y == i, :].min() * (_max - _min) + _min\n",
    "        vmax = X[y == i, :].max() * (_max - _min) + _min\n",
    "        norm = matplotlib.colors.Normalize(vmin=VMIN[j], vmax=VMAX[j])\n",
    "        \n",
    "        # Gets the maxima values of features at each coordinates\n",
    "        maxima_values[KEYS[kf]] = grid_stats[coordinates]\n",
    "        \n",
    "        # Converts maxima values to rgb color\n",
    "        color_values[KEYS[kf]] = [\"{}, {}, {}\".format(*(c[:3] * 255).astype(int)) for c in CMAP((grid_stats[coordinates] - VMIN[j]) / (VMAX[j] - VMIN[j]))]\n",
    "        \n",
    "        # Plots local mean feature, KDE contour and local maxima\n",
    "        fig, ax = pyplot.subplots(figsize=(7, 6))\n",
    "        ax.set_title(KEYS[kf])\n",
    "        xpts, ypts = xx[coordinates], yy[coordinates]\n",
    "        trans_offset = matplotlib.transforms.offset_copy(ax.transData, fig=fig, x=0.05, y=0.05)\n",
    "        for _id, (xpt, ypt) in enumerate(zip(xpts, ypts)):\n",
    "            ax.scatter(xpt, ypt, s=25, c=\"darkred\")\n",
    "            ax.text(xpt, ypt, str(_id), transform=trans_offset)\n",
    "        pyplot.colorbar(matplotlib.cm.ScalarMappable(norm=norm, cmap=CMAP), ax=ax)\n",
    "        ax.imshow(grid_stats, origin=\"lower\", extent=(*grid.xlim, *grid.ylim),\n",
    "                    aspect=\"auto\", cmap=CMAP, norm=norm)\n",
    "        ax.contour(cf, colors=\"k\", levels=levels[:])\n",
    "        ax.set_ylim(ylim)\n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_xlabel(\"UMAP 1\")\n",
    "        ax.set_ylabel(\"UMAP 2\")\n",
    "        pyplot.show()\n",
    "        \n",
    "        # Clears grid \n",
    "        grid.clear_grid()\n",
    "    \n",
    "    # Keeps track of maxima values and color values\n",
    "    total_maximas.append(maxima_values)\n",
    "    total_colors.append(color_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hierarchical-clustering'></a>\n",
    "## Hierarchical clustering\n",
    "\n",
    "In this section we perform the hierarchical clustering of the synaptic subtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Defines the keys order\n",
    "keys_order = [6, 1, 0, 2, 3, 4, 5]\n",
    "\n",
    "# Combines the maximas in a single numpy.ndarray \n",
    "maximas, labels = defaultdict(list), []\n",
    "for (_dict, (plasticity, cond)) in zip(total_maximas, CONDITIONS):\n",
    "    labels.extend(f\"{cond}({i})\" for i in range(list(_dict.values())[0].size))\n",
    "    for key, values in _dict.items():\n",
    "        maximas[key].extend(values)\n",
    "maximas = numpy.array(list(maximas.values())).T\n",
    "\n",
    "# Combines colors in a single numpy.ndarray\n",
    "_colors = defaultdict(list)\n",
    "for _dict in total_colors:\n",
    "    for key, values in _dict.items():\n",
    "        _colors[key].extend([list(map(int, v.split(\", \"))) for v in values])\n",
    "_colors = numpy.transpose(numpy.array(list(_colors.values())), axes=(1, 0, 2))\n",
    "\n",
    "# Clustering\n",
    "_X = (maximas - _min) / (_max - _min)\n",
    "clustering = AgglomerativeClustering(n_clusters=12 if CHAN == 1 else 8, \n",
    "                                        affinity=\"euclidean\", linkage=\"ward\").fit(_X)\n",
    "\n",
    "# Hierchiarchical clustering\n",
    "children = clustering.children_\n",
    "dist = numpy.arange(children.shape[0])\n",
    "no_of_observations = numpy.arange(2, children.shape[0]+2)\n",
    "Z = numpy.column_stack([children, dist, no_of_observations]).astype(float)\n",
    "hierarchy.set_link_color_palette([\"k\"])\n",
    "labels = [\", \".join((str(l2), str(l1))) for l1, l2 in zip(labels, clustering.labels_)]\n",
    "\n",
    "# Plots dendogram\n",
    "gs = gridspec.GridSpec(1, 3, height_ratios=[1])\n",
    "fig = pyplot.figure(figsize=(15, 10))\n",
    "axes = [pyplot.subplot(_gs) for _gs in gs]\n",
    "dn = hierarchy.dendrogram(Z, ax=axes[0], labels=labels, orientation=\"left\", above_threshold_color='k', color_threshold=15)\n",
    "\n",
    "# Average same cluster synapse\n",
    "clabel = []\n",
    "for cl in clustering.labels_[dn[\"leaves\"]]:\n",
    "    if cl not in clabel:\n",
    "        clabel.append(cl)\n",
    "average_cluster, mean_clustering_labels = [], []\n",
    "for cl in clabel:\n",
    "    averages = numpy.mean(maximas[clustering.labels_ == cl], axis=0)\n",
    "    mean_clustering_labels.append(averages)\n",
    "    avg_cluster = []\n",
    "    for i, avg in enumerate(averages):\n",
    "        norm = matplotlib.colors.Normalize(vmin=VMIN[i], vmax=VMAX[i])\n",
    "        scalarmap = matplotlib.cm.ScalarMappable(norm=norm, cmap=CMAP)\n",
    "        avg_cluster.append(scalarmap.to_rgba(avg))\n",
    "    average_cluster.append(avg_cluster)\n",
    "average_cluster = numpy.array(average_cluster)\n",
    "average_cluster = average_cluster[:, keys_order, :]\n",
    "\n",
    "# Plots averaged same cluster synapses\n",
    "axes[2].imshow(average_cluster, origin=\"lower\")\n",
    "axes[1].imshow(_colors[dn[\"leaves\"]][:, keys_order, :], origin=\"lower\")\n",
    "xticklabels = numpy.array(KEEP_FEATURES)[keys_order]\n",
    "xticklabels = numpy.array(KEYS)[xticklabels]\n",
    "axes[2].set_xticks(numpy.arange(len(xticklabels)))\n",
    "axes[2].set_xticklabels(xticklabels, rotation=30, horizontalalignment=\"right\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='associate-cluster'></a>\n",
    "## Associate clusters\n",
    "\n",
    "We associate each cluster to a subtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output = defaultdict(dict)\n",
    "for (plasticity, cond) in CONDITIONS:\n",
    "    density_per_image, association_distance = associate(maximas, clustering.labels_, plasticity, cond, CHAN, KEEP_FEATURES)\n",
    "    output[\"density\"][cond] = density_per_image\n",
    "    output[\"distances\"][cond] = association_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
